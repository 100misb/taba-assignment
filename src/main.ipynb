{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.corpus\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import List\n",
    "from functools import reduce\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Any, Union\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/somi/ampba/taba/taba-\n",
      "[nltk_data]     assignment/src...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "nb_full_path = os.path.join(os.getcwd())\n",
    "nltk.download('wordnet',download_dir=nb_full_path)\n",
    "nltk.data.path.append(nb_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide proper path to your dataset\n",
    "PATH = \"../data/uber_reviews_itune.csv\"\n",
    "STOPWORDS = stopwords.words(\"english\")\n",
    "STEMMER = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH, encoding=\"cp1252\", delimiter=\",\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning \n",
    "1. Performed keeping these pointers in mind : \n",
    "   1. Normalize Text\n",
    "   2. Remove unicode characters\n",
    "   3. Remove Stopwords\n",
    "   4. Perform Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    try: \n",
    "        normalized_text = text.lower()\n",
    "    except Exception as e :\n",
    "        print(f\"Error while normalizing text `{text}` , due to : {e}\")\n",
    "        return \"\"\n",
    "    else:\n",
    "        # print(f\"normalize_text : {normalized_text}\")\n",
    "        return normalized_text\n",
    "\n",
    "def remove_unicode_characters(text:str) -> str:\n",
    "    unicode_free_text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text)\n",
    "    # print(f\"remove_unicode_char : {unicode_free_text}\")\n",
    "    return unicode_free_text\n",
    "\n",
    "def remove_stopwords(text: str, stops: List[str] = STOPWORDS) -> str:\n",
    "    filtered_text = \" \".join(\n",
    "        [word for word in text.split() if word not in stops]\n",
    "    )\n",
    "    # print(f\"remove_stopwords: {filtered_text}\")\n",
    "    return filtered_text\n",
    "\n",
    "def tokenize_and_stem(text, lemmatizer = lemmatizer) -> List[str]:\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation) using regex\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [lemmatizer.lemmatize(t) for t in filtered_tokens]\n",
    "    # print(f\"tokenize : {stems}\")\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_func_chain(*functions):\n",
    "    return reduce(lambda f, g: lambda x: g(f(x)), functions, lambda x: x)\n",
    "\n",
    "# chain list of operation together\n",
    "clean_text = compose_func_chain(\n",
    "    normalize_text,\n",
    "    remove_unicode_characters,\n",
    "    remove_stopwords\n",
    ")\n",
    "\n",
    "def clean_text_wrapper(iterable:Iterable[Union[str, Any]], compose_func) -> List[List[str]]:\n",
    "    cleaned_corpus = list(\n",
    "        map(\n",
    "            compose_func, iterable\n",
    "        )\n",
    "    )\n",
    "    return cleaned_corpus\n",
    "\n",
    "corpus = clean_text_wrapper(\n",
    "    df[\"Review\"], clean_text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/somi/ampba/taba/taba-assignment/.venv/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(490, 2890)\n"
     ]
    }
   ],
   "source": [
    "# defining parms for the tfidf-tokenizer here\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    use_idf=True, \n",
    "    tokenizer=tokenize_and_stem, \n",
    ")\n",
    "\n",
    "# note magic cmd %time\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)    # 6.05 secs\n",
    "\n",
    "print(tfidf_matrix.shape)    # dimns of the tfidf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['write', 'writing', 'wrong', 'wrote', 'x', 'xoxo', 'ya', 'yall', 'yard', 'yea', 'yeah', 'year', 'yeeesh', 'yelling', 'yes', 'yesterday', 'yk', 'yo', 'york', 'youd', 'yougood', 'youll', 'young', 'youre', 'youve', 'youworst', 'yu', 'zero', 'zone', 'zoom']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "print(terms[-30:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "279a066a14feccc0c7f1214bbde4e4105945e2187a185c51e0edd69df4ea57fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
